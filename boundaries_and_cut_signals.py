#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import json
import numpy as np
from obspy import read, Trace, Stream
# from scipy.stats import deprmsg


#from main import LOG_DIR

#from scipy.stats import pairs



def load_json(path):
    if os.path.exists(path):
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return {}
    return {}


def save_json(path, data):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)

def extract_segment_from_mseed_file(input_path: str, start_index: int, duration_samples: int):
    try:
        st = read(input_path)
        trimmed = st.copy().clear()

        for tr in st:
            end_index = start_index + duration_samples
            if end_index > len(tr.data):
                print(f"âš ï¸ Î ÎµÏÎ¹Î¿ÏÎ¹ÏƒÎ¼ÏŒÏ‚ end_index ÏƒÏ„Î¿ Î¼Î®ÎºÎ¿Ï‚ Ï„Î¿Ï… trace ({len(tr.data)})")
                end_index = len(tr.data)

            segment = tr.data[start_index:end_index].astype(np.float32)
            if not np.all(np.isfinite(segment)):
                print(f"âš ï¸ ÎœÎ· Î­Î³ÎºÏ…ÏÎµÏ‚ Ï„Î¹Î¼Î­Ï‚ (NaN/Inf) ÏƒÏ„Î¿ {tr.id}")
                return None

            segment = np.clip(segment, -1e12, 1e12)
            seg_trace = tr.copy()
            seg_trace.data = segment
            seg_trace.stats.npts = len(segment)
            seg_trace.stats.starttime += start_index / seg_trace.stats.sampling_rate
            trimmed += seg_trace

        folder = os.path.dirname(input_path)
        base = os.path.basename(input_path).replace(".mseed", "")
        output_filename = f"{base}_PS.mseed"
        output_path = os.path.join(folder, output_filename)

        trimmed.write(output_path, format="MSEED")
        return output_path

    except Exception as e:
        print(f"âŒ Î£Ï†Î¬Î»Î¼Î± ÏƒÏ„Î¿ extract_segment_from_mseed_file: {e}")
        return None

from typing import List, Set, Tuple

# ==========================================================
# âœ… Î¦Î‘Î£Î— 1: Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ start/pick/end & ÎµÎ½Î·Î¼Î­ÏÏ‰ÏƒÎ· JSON
# ==========================================================
def find_boundaries():
    import os
    import json
    import numpy as np
    from obspy import read
    from scipy.signal import find_peaks, butter, filtfilt, hilbert
    from main import LOG_DIR, BASE_DIR

    # --- Helper: Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· JSON Î¼Îµ atomic Ï„ÏÏŒÏ€Î¿ ---
    def save_json(path, data):
        tmp_path = path + ".tmp"
        with open(tmp_path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        os.replace(tmp_path, path)

    def load_json(path):
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)

    # --- Helper: Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ ÎµÎ»Î¬Ï‡Î¹ÏƒÏ„Î¿Ï… SNR ---
    def add_min_station_snr(station_results: dict, minimum_station_snr: float):
        max_duration = 0.0
        for ch, info in station_results.items():
            if isinstance(info, dict) and "duration_time" in info:
                try:
                    dur = float(info["duration_time"])
                    if dur > max_duration:
                        max_duration = dur
                except ValueError:
                    continue
        station_results["minimum_station_snr"] = round(minimum_station_snr, 3)

    # --- Bandpass Ï†Î¯Î»Ï„ÏÎ¿ 1â€“20 Hz ---
    def bandpass_filter(data, sr, fmin=1.0, fmax=20.0, order=4):
        nyquist = 0.5 * sr
        low = fmin / nyquist
        high = fmax / nyquist
        b, a = butter(order, [low, high], btype="band")
        return filtfilt(b, a, data)

    # --- Paths ---
    OUTPUT_JSON = os.path.join(LOG_DIR, "boundaries.json")
    AIC_FAIL_JSON = os.path.join(LOG_DIR, "AIC_failure.json")
    os.makedirs(LOG_DIR, exist_ok=True)

    snr_path = os.path.join(LOG_DIR, "snr.json")
    if not os.path.exists(snr_path):
        print(f"âŒ Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ Ï„Î¿ {snr_path}")
        return set()

    try:
        snr_data = load_json(snr_path)
    except Exception as e:
        print(f"âš ï¸ Î£Ï†Î¬Î»Î¼Î± ÎºÎ±Ï„Î¬ Ï„Î·Î½ Î±Î½Î¬Î³Î½Ï‰ÏƒÎ· Ï„Î¿Ï… snr.json: {e}")
        return set()

    # --- Î‘Î½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ Î®Î´Î· Ï„Î¿ boundaries.json, Ï†ÏŒÏÏ„Ï‰ÏƒÎ­ Ï„Î¿ ---
    if os.path.exists(OUTPUT_JSON):
        try:
            all_results = load_json(OUTPUT_JSON)
            print(f"ğŸ“‚ Î¦Î¿ÏÏ„ÏÎ¸Î·ÎºÎµ Ï„Î¿ Ï…Ï€Î¬ÏÏ‡Î¿Î½ boundaries.json")
        except Exception:
            all_results = {}
    else:
        all_results = {}

    events_dict = snr_data.get("Events", {})

    # --- ÎšÏÏÎ¹Î¿Ï‚ Î²ÏÏŒÏ‡Î¿Ï‚ ---
    for year, events in events_dict.items():
        for eventJson, stations in events.items():
            for stationJson, chans in stations.items():
                # Skip Î±Î½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ Î®Î´Î·
                if (
                    str(year) in all_results
                    and eventJson in all_results[str(year)]
                    and stationJson in all_results[str(year)][eventJson]
                ):
                    print(f"â­ï¸ Î Î±ÏÎ¬Î»ÎµÎ¹ÏˆÎ·: {year}/{eventJson}/{stationJson} Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ Î®Î´Î·")
                    continue

                year_path = os.path.join(BASE_DIR, year)
                event_path = os.path.join(year_path, eventJson)
                station_path = os.path.join(event_path, stationJson)
                station_results = {}

                # SNR Ï„Î¿Ï… ÏƒÏ„Î±Î¸Î¼Î¿Ï
                station_snr = chans.get("minimum_snr", 0)

                for root, _, files in os.walk(station_path):
                    if "info.json" in root:
                        continue
                    for fname in files:
                        if not fname.endswith("_demeanDetrend_IC_BPF.mseed") or "HHZ" not in fname:
                            continue
                        try:
                            st = read(os.path.join(station_path, fname))
                        except Exception as e:
                            print(f"âš ï¸ Î‘Ï€Î¿Ï„Ï…Ï‡Î¯Î± Î±Î½Î¬Î³Î½Ï‰ÏƒÎ·Ï‚ {year}/{eventJson}/{stationJson} {fname}: {e}")
                            continue

                        channelSnrJson = chans.get("HHZ", 0).get("snr", 0)

                        for tr in st:
                            try:
                                data = tr.data.astype(float)
                                sr = tr.stats.sampling_rate

                                # --- Î’Î®Î¼Î± 1: Î•ÏÏÎµÏƒÎ· AIC Î­Î½Î±ÏÎ¾Î·Ï‚ ---
                                start_of_event_idx, _ = aic_picker(data)
                                if start_of_event_idx is None:
                                    # --- ÎšÎ±Ï„Î±Î³ÏÎ±Ï†Î® AIC Î±Ï€Î¿Ï„Ï…Ï‡Î¯Î±Ï‚ (ÎÎ•Î‘ Î”ÎŸÎœÎ—) ---
                                    try:
                                        aic_failures = load_json(AIC_FAIL_JSON) if os.path.exists(AIC_FAIL_JSON) else {}
                                    except Exception:
                                        aic_failures = {}

                                    # root counter
                                    current_count = aic_failures.get("count", 0)

                                    # year â†’ event â†’ station (no channel)
                                    year_dict = aic_failures.setdefault(str(year), {})
                                    event_dict = year_dict.setdefault(eventJson, {})

                                    # Î±Î½ Î¿ ÏƒÏ„Î±Î¸Î¼ÏŒÏ‚ Î´ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ Î®Î´Î·, Ï€ÏÏŒÏƒÎ¸ÎµÏƒÎ­ Ï„Î¿Î½ ÎºÎ±Î¹ Î±ÏÎ¾Î·ÏƒÎµ count
                                    if stationJson not in event_dict:
                                        event_dict[stationJson] = True
                                        aic_failures["count"] = current_count + 1

                                    save_json(AIC_FAIL_JSON, aic_failures)
                                    continue

                                # --- Î’Î®Î¼Î± 2: Bandpass 1â€“20 Hz ---
                                filtered = bandpass_filter(data, sr, 1.0, 20.0)

                                # --- Î’Î®Î¼Î± 3: Hilbert envelope ---
                                envelope = np.abs(hilbert(filtered))
                                norm_env = envelope / (np.max(envelope) or 1.0)

                                # --- Î’Î®Î¼Î± 4: Buffer 0.5 s Î¼ÎµÏ„Î¬ Ï„Î¿ AIC ---
                                buffer_samples = int(0.5 * sr)
                                search_segment = norm_env[start_of_event_idx + buffer_samples:]
                                threshold = 0.2 * np.max(search_segment)

                                # --- Î’Î®Î¼Î± 5: Î•ÏÏÎµÏƒÎ· peaks ---
                                peaks, properties = find_peaks(
                                    search_segment,
                                    height=threshold,
                                    prominence=0.8,
                                    distance=int(0.3 * sr)
                                )
                                if len(peaks) == 0:
                                    peak_amplitude_idx = int(start_of_event_idx + np.argmax(search_segment))
                                else:
                                    peak_amplitude_idx = peaks[0]
                                    peak_amplitude_idx = peak_amplitude_idx + start_of_event_idx
                                    # peak_amplitude_idx = start_of_event_idx + buffer_samples + main_peak

                                # --- Î’Î®Î¼Î± 6: Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ Ï‡ÏÏŒÎ½Ï‰Î½ ---
                                start_of_event_datetime = tr.stats.starttime + start_of_event_idx / sr
                                peak_amplitude_datetime = tr.stats.starttime + peak_amplitude_idx / sr
                                end_of_peak_segment_idx = 2 * peak_amplitude_idx - start_of_event_idx
                                end_of_peak_segment_datetime = tr.stats.starttime + end_of_peak_segment_idx / sr
                                peak_segment_duration_samples = int(2 * (peak_amplitude_idx - start_of_event_idx))
                                peak_segment_duration_time = peak_segment_duration_samples / sr

                                pick_ampl = float(norm_env[peak_amplitude_idx])
                                ch_id = tr.id.split('.')[-1]

                                # --- Î’Î®Î¼Î± 7: Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ Ï„Î­Î»Î¿Ï…Ï‚ ÏƒÎ®Î¼Î±Ï„Î¿Ï‚ Î¼Îµ Î²Î¬ÏƒÎ· Ï„Î¿ SNR ---
                                threshold_end = 1.0 / (channelSnrJson or 1.0)
                                end_of_event_idx = None
                                for i in range(peak_amplitude_idx, len(norm_env)):
                                    if norm_env[i] <= threshold_end:
                                        end_of_event_idx = i
                                        break
                                if end_of_event_idx is None:
                                    end_of_event_idx = len(norm_env) - 1

                                end_of_event_idx = end_of_event_idx + start_of_event_idx
                                end_of_event_time = tr.stats.starttime + end_of_event_idx / sr

                                # --- Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ ÏƒÏ…Î½Î¿Î»Î¹ÎºÎ®Ï‚ Î´Î¹Î¬ÏÎºÎµÎ¹Î±Ï‚ ---
                                clean_event_duration_nof_samples = int(end_of_event_idx) - int(start_of_event_idx)
                                clean_event_duration_time = clean_event_duration_nof_samples / sr
                                total_signal_nof_samples = len(tr.data)
                                total_signal_time = total_signal_nof_samples / sr

                                # --- Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· ---
                                station_results[ch_id] = {
                                    "start_of_event_idx": int(start_of_event_idx),
                                    "start_of_event_datetime": str(start_of_event_datetime),
                                    "peak_amplitude_idx": int(peak_amplitude_idx),
                                    "peak_amplitude_datetime": str(peak_amplitude_datetime),
                                    "peak_amplitude": round(pick_ampl, 5),
                                    "end_of_peak_segment_idx": int(end_of_peak_segment_idx),
                                    "end_of_peak_segment_datetime": str(end_of_peak_segment_datetime),
                                    "peak_segment_duration_nof_samples": int(peak_segment_duration_samples),
                                    "peak_segment_duration_time": f"{peak_segment_duration_time:.2f}",
                                    "end_of_event_idx": int(end_of_event_idx),
                                    "end_of_event_time": str(end_of_event_time),
                                    "clean_event_duration_nof_samples": int(clean_event_duration_nof_samples),
                                    "clean_event_duration_time": f"{clean_event_duration_time:.2f}",
                                    "total_signal_nof_samples": int(total_signal_nof_samples),
                                    "total_signal_time": f"{total_signal_time:.2f}",
                                }

                            except Exception as e:
                                print(f"âš ï¸ Î£Ï†Î¬Î»Î¼Î± ÏƒÏ„Î¿ {year}/{eventJson}/{stationJson}/{tr.id}: {e}")

                # âœ… ÎŸ ÏƒÏ„Î±Î¸Î¼ÏŒÏ‚ Î¿Î»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ
                if len(station_results) > 0:
                    add_min_station_snr(station_results, station_snr)

                    # --- ÎœÎµÏ„ÏÎ·Ï„Î®Ï‚ ÏƒÏ„Î±Î¸Î¼ÏÎ½ ---
                    total_key = "total_nof_stations"
                    prev = all_results.get(total_key, 0)
                    all_results[total_key] = prev + 1

                    # --- Î•Î½Î·Î¼Î­ÏÏ‰ÏƒÎ· Î´Î¿Î¼Î®Ï‚ ---
                    year_dict = all_results.setdefault(str(year), {})
                    event_dict = year_dict.setdefault(eventJson, {})
                    event_dict[stationJson] = station_results

                    save_json(OUTPUT_JSON, all_results)

                    print(
                        f"ğŸ’¾ Î‘Ï€Î¿Î¸Î·ÎºÎµÏÏ„Î·ÎºÎµ {year}/{eventJson}/{stationJson}: "
                        f"SNR={station_snr:.2f}, peak_segment_duration_time={peak_segment_duration_time:.2f}, "
                        f"clean_event_duration_time={clean_event_duration_time:.2f}, total_signal_time={total_signal_time:.2f}"
                    )

    print(f"\nâœ… ÎŸÎ»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ Î· ÎºÎ±Ï„Î±Î³ÏÎ±Ï†Î® ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ ÏƒÏ„Î±Î¸Î¼ÏÎ½ ÏƒÏ„Î¿: {OUTPUT_JSON}")



# ==========================================================
# âœ… Î¦Î‘Î£Î— 2: Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î±Ï€Î¿ÏƒÏ€Î±ÏƒÎ¼Î¬Ï„Ï‰Î½ Î¼Îµ ÏƒÏ„Î±Î¸ÎµÏÏŒ duration
# ==========================================================
def create_cutted_signal_files(min_snr: float, max_ps_duration: float):
    """
    Î”Î·Î¼Î¹Î¿Ï…ÏÎ³ÎµÎ¯ PS Î±ÏÏ‡ÎµÎ¯Î± (peak-segmented MiniSEED) Î±Ï€ÏŒ Ï„Î± BPF Î±ÏÏ‡ÎµÎ¯Î±.

    Î•Ï€Î¹Î»Î­Î³ÎµÎ¹ Î¼ÏŒÎ½Î¿:
        - minimum_station_snr >= min_snr
        - peak_segment_duration_time <= max_ps_duration

    Î¤Î¿ Î½Î­Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ ÎºÏŒÎ²ÎµÏ„Î±Î¹ Î±Ï€ÏŒ:
        start_of_event_idx  -->  end_of_peak_segment_idx

    ÎšÎ±Î¹ Î±Ï€Î¿Î¸Î·ÎºÎµÏÎµÏ„Î±Î¹ Ï‰Ï‚:
        <station>..<channel>__<event>_demeanDetrend_IC_BPF__PS__duration_LE_<max_ps_duration>__SNR_GE_<min_snr>.mseed
    """

    import os
    from obspy import read
    from main import LOG_DIR, BASE_DIR

    PS_JSON = os.path.join(LOG_DIR, "boundaries.json")
    if not os.path.exists(PS_JSON):
        print(f"âŒ Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ Ï„Î¿ {PS_JSON}")
        return

    db = load_json(PS_JSON)

    # --------------------------------------------------------------------
    # Loop ÏƒÎµ ÏŒÎ»Î± Ï„Î± events / stations / channels
    # --------------------------------------------------------------------
    for year, events in db.items():
        if year == "total_nof_stations":
            continue

        for event_name, stations in events.items():
            for station_name, channels in stations.items():

                # --- Station-level SNR ---
                station_snr = channels.get("minimum_station_snr", 0)
                if station_snr < min_snr:
                    continue

                # Path Ï„Î¿Ï… station
                station_path = os.path.join(BASE_DIR, str(year), event_name, station_name)
                if not os.path.exists(station_path):
                    print(f"âš ï¸ Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ Î¿ Ï†Î¬ÎºÎµÎ»Î¿Ï‚: {station_path}")
                    continue

                # Output subfolder
                output_dir = os.path.join(
                    station_path,
                    f"PS_SNR_GE_{min_snr}__DUR_LE_{max_ps_duration}"
                )
                os.makedirs(output_dir, exist_ok=True)

                # ----------------------------------------------------------------
                # Loop ÏƒÎµ channels
                # ----------------------------------------------------------------
                for ch_name, ch_info in channels.items():
                    if not isinstance(ch_info, dict):
                        continue
                    if not ch_name.endswith("Z"):
                        continue

                    # Duration filter
                    ps_duration = ch_info.get("peak_segment_duration_time")
                    if ps_duration is None:
                        continue

                    try:
                        ps_duration = float(ps_duration)
                    except:
                        continue

                    if ps_duration > max_ps_duration:
                        continue

                    # Required boundaries
                    start_idx = ch_info.get("start_of_event_idx")
                    end_peak_idx = ch_info.get("end_of_peak_segment_idx")

                    if start_idx is None or end_peak_idx is None:
                        continue

                    # ----------------------------------------------------------------
                    # Original file path
                    # ----------------------------------------------------------------
                    orig_file = os.path.join(
                        station_path,
                        f"{station_name}..{ch_name}__{event_name}_demeanDetrend_IC_BPF.mseed"
                    )

                    if not os.path.exists(orig_file):
                        print(f"âš ï¸ Missing original file: {orig_file}")
                        continue

                    # ----------------------------------------------------------------
                    # Read & cut waveform
                    # ----------------------------------------------------------------
                    try:
                        st = read(orig_file)
                        tr = st[0]
                        data = tr.data
                        sr = tr.stats.sampling_rate

                        segment = data[start_idx:end_peak_idx]
                        if len(segment) == 0:
                            print(f"âš ï¸ Empty segment in {orig_file}")
                            continue

                        # Create new stream
                        new_tr = tr.copy()
                        new_tr.data = segment
                        new_st = Stream([new_tr])

                    except Exception as e:
                        print(f"âš ï¸ Error reading {orig_file}: {e}")
                        continue

                    # ----------------------------------------------------------------
                    # Construct output filename
                    # ----------------------------------------------------------------
                    out_name = (
                        f"{station_name}..{ch_name}__{event_name}"
                        f"_demeanDetrend_IC_BPF__PS__duration_LE_{max_ps_duration}"
                        f"__SNR_GE_{min_snr}.mseed"
                    )
                    out_path = os.path.join(output_dir, out_name)

                    # Save output
                    try:
                        new_st.write(out_path, format="MSEED")
                        print(f"âœ… Created: {out_path}")
                    except Exception as e:
                        print(f"âŒ Error writing {out_path}: {e}")

    print(f"\nğŸ‰ Completed PS file creation for SNR â‰¥ {min_snr}, PS_duration â‰¤ {max_ps_duration}s.\n")


def aic_picker(trace_data):
    """
    Î¥Ï€Î¿Î»Î¿Î³Î¯Î¶ÎµÎ¹ Ï„Î¿ AIC ÏƒÎµ Î¿Î»ÏŒÎºÎ»Î·ÏÎ¿ Ï„Î¿ ÏƒÎ®Î¼Î± (Î¼Î­Ï‡ÏÎ¹ Ï„Î¿ pick) ÎºÎ±Î¹ ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹
    Ï„Î¿ index ÏŒÏ€Î¿Ï… ÎµÎ»Î±Ï‡Î¹ÏƒÏ„Î¿Ï€Î¿Î¹ÎµÎ¯Ï„Î±Î¹, Ï‰Ï‚ Ï€Î¹Î¸Î±Î½Î® Î­Î½Î±ÏÎ¾Î· Ï„Î¿Ï… ÏƒÎµÎ¹ÏƒÎ¼Î¹ÎºÎ¿Ï ÎºÏÎ¼Î±Ï„Î¿Ï‚.

    :param trace_data: numpy array Î¼Îµ Ï„Î¿ ÏƒÎµÎ¹ÏƒÎ¼Î¹ÎºÏŒ ÏƒÎ®Î¼Î± (float, demeaned)
    :return: (index_Î­Î½Î±ÏÎ¾Î·Ï‚, ÎºÎ±Î¼Ï€ÏÎ»Î·_AIC)
    """
    data = trace_data.astype(float)
    n = len(data)
    if n < 3:
        return None, np.array([])

    pick_idx = int(np.argmax(np.abs(data)))  # Î¼Î­Î³Î¹ÏƒÏ„Î· Î±Ï€ÏŒÎ»Ï…Ï„Î· Ï„Î¹Î¼Î®
    if pick_idx < 10:
        return None, np.array([])  # Ï€Î¿Î»Ï Î¼Î¹ÎºÏÏŒ ÏƒÎ®Î¼Î±

    aic = np.zeros(pick_idx)

    for k in range(1, pick_idx - 1):
        var1 = np.var(data[:k]) or 1e-10
        var2 = np.var(data[k:pick_idx]) or 1e-10
        aic[k] = k * np.log(var1) + (pick_idx - k - 1) * np.log(var2)

    min_idx = int(np.argmin(aic[1:pick_idx - 1])) + 1
    return min_idx, aic






def count_nof_training_stations_and_create_json_files(
        min_snr: float,
        max_ps_duration: float,
        depthMin: float,
        depthMax: float):

    """
    Classifies Z-channel signals from boundaries.json into:

    A) Training-eligible:
         - minimum_station_snr >= min_snr
         - peak_segment_duration_time <= max_ps_duration
         - clean_event_duration_time >= min_clean_event_duration
         - depthMin <= Depth <= depthMax

    B1) High SNR (>= min_snr) & TOO LONG peak segment (> max_ps_duration s) and depth in bounds
    B2) Low SNR (< min_snr)
    B3) High SNR (>= min_snr) & TOO SHORT clean event (< min_clean_event_duration s)
    B4) High SNR (>= min_snr) & (Depth < depthMin km or Depth > depthMax km)

    Produces:
      â€¢ trainingSet_SNR_GE_<min_snr>_PS_duration_LE_<max_ps_duration>.json
      â€¢ PotentiallyUsedOnTrainingSet_SNR_GE_<min_snr>_PS_duration_GE_<max_ps_duration>.json
    """

    import os
    from main import LOG_DIR, BASE_DIR

    # Threshold Î³Î¹Î± clean_event_duration (Î¯ÏƒÎ¿ Î¼Îµ max_ps_duration ÏŒÏ€Ï‰Ï‚ Ï€ÏÎ¹Î½)
    min_clean_event_duration = max_ps_duration

    # --- Paths ---
    json_path = os.path.join(LOG_DIR, "boundaries.json")

    if not os.path.exists(json_path):
        print(f"âŒ File not found: {json_path}")
        return

    data = load_json(json_path)

    # --- New JSON files ---
    training_json = {}
    potential_json = {}

    # --- Counters ---
    to_training = 0
    high_snr_and_high_ps_duration_and_depth_in_bounds = 0  # B1
    low_snr = 0                        # B2
    high_snr_but_low_clean_event = 0   # B3
    depth_out_of_range = 0             # B4

    # --- Traverse structure: year â†’ event â†’ station â†’ channel ---
    for year, events in data.items():

        if year == "total_nof_stations":
            continue
        if not isinstance(events, dict):
            continue

        for event_name, stations in events.items():
            if not isinstance(stations, dict):
                continue

            # --------------------------------------------------
            # Î’Î¡Î•Î£ Î¤ÎŸ Î’Î‘Î˜ÎŸÎ£ Î¤ÎŸÎ¥ EVENT Î‘Î ÎŸ info.json
            # --------------------------------------------------
            depth_km = None
            info_path = os.path.join(BASE_DIR, str(year), event_name, "info.json")
            if os.path.exists(info_path):
                try:
                    info_data = load_json(info_path)
                    # Î ÏÎ¿ÏƒÏ€Î¬Î¸Î·ÏƒÎµ Î¼Îµ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ¬ Ï€Î¹Î¸Î±Î½Î¬ ÎºÎ»ÎµÎ¹Î´Î¹Î¬
                    depth_km = (
                        info_data.get("Depth_km")
                        or info_data.get("Depth-km")
                        or info_data.get("depth_km")
                        or info_data.get("depth-km")
                    )
                    if depth_km is not None:
                        depth_km = float(depth_km)
                except Exception as e:
                    print(f"âš ï¸ Could not read depth from {info_path}: {e}")
                    depth_km = None

            for station_name, channels in stations.items():
                if not isinstance(channels, dict):
                    continue

                # Station SNR
                station_snr = channels.get("minimum_station_snr")
                if station_snr is None:
                    continue
                station_snr = float(station_snr)

                # --- For every Z channel ---
                for ch_name, ch_info in channels.items():

                    if not isinstance(ch_info, dict):
                        continue
                    if not ch_name.endswith("Z"):
                        continue

                    # --- Peak Segmentation Duration ---
                    ps_dur = ch_info.get("peak_segment_duration_time")
                    if ps_dur is None:
                        continue
                    try:
                        ps_dur = float(ps_dur)
                    except:
                        continue

                    # --- Clean Event Duration ---
                    clean_dur = ch_info.get("clean_event_duration_time")
                    if clean_dur is None:
                        continue
                    try:
                        clean_dur = float(clean_dur)
                    except:
                        continue

                    # ------------------------------------------------------
                    # CATEGORY B2 â€” LOW SNR (< min_snr)
                    # ------------------------------------------------------
                    if station_snr < min_snr:
                        low_snr += 1
                        continue

                    # ------------------------------------------------------
                    # CATEGORY B1 â€” HIGH SNR but PS too long
                    #   (Î‘Ï…Ï„Î¬ Î¼Ï€Î±Î¯Î½Î¿Ï…Î½ ÎºÎ±Î¹ ÏƒÏ„Î¿ PotentiallyUsedOnTrainingSet.json)
                    #   Î ÏÎ¿ÏƒÎ¿Ï‡Î®: ÎµÎ´Ï Î”Î•Î ÎµÎ»Î­Î³Ï‡Î¿Ï…Î¼Îµ depth.
                    # ------------------------------------------------------
                    if ps_dur > max_ps_duration:
                        # Î¥Ï€Î¿ÏˆÎ®Ï†Î¹Î¿Ï‚ ÎœÎŸÎÎŸ Î±Î½ depth ÎµÎ¯Î½Î±Î¹ Î³Î½Ï‰ÏƒÏ„ÏŒ ÎºÎ±Î¹ ÎµÎ½Ï„ÏŒÏ‚ Î¿ÏÎ¯Ï‰Î½
                        if depth_km is not None and depthMin <= depth_km <= depthMax:
                            high_snr_and_high_ps_duration_and_depth_in_bounds += 1

                            year_dict = potential_json.setdefault(year, {})
                            event_dict = year_dict.setdefault(event_name, {})
                            event_dict[station_name] = channels

                        # Î‘Î½ depth ÎµÎ¯Î½Î±Î¹ None Î® ÎµÎºÏ„ÏŒÏ‚ Î¿ÏÎ¯Ï‰Î½ â†’ Ï„ÏŒÏ„Îµ Ï€Î¬ÎµÎ¹ ÏƒÏ„Î¿ B4
                        else:
                            depth_out_of_range += 1

                        continue

                    # ------------------------------------------------------
                    # CATEGORY B4 â€” HIGH SNR & DEPTH OUT OF RANGE
                    #   Depth < depthMin Î® Depth > depthMax
                    #   Î‘Î½ depth_km is None â†’ Î¸ÎµÏ‰ÏÎµÎ¯Ï„Î±Î¹ ÎµÎºÏ„ÏŒÏ‚ Î¿ÏÎ¯Ï‰Î½
                    # ------------------------------------------------------
                    if depth_km is None or depth_km < depthMin or depth_km > depthMax:
                        depth_out_of_range += 1
                        continue

                    # ------------------------------------------------------
                    # CATEGORY B3 â€” HIGH SNR but Clean Event too short
                    # ------------------------------------------------------
                    if clean_dur < min_clean_event_duration:
                        high_snr_but_low_clean_event += 1
                        continue

                    # ------------------------------------------------------
                    # CATEGORY A â€” Training-eligible
                    # ------------------------------------------------------
                    if depthMin <= depth_km <= depthMax:
                        to_training += 1

                    year_dict = training_json.setdefault(year, {})
                    event_dict = year_dict.setdefault(event_name, {})
                    event_dict[station_name] = channels

    # ----------------------------------------------------------
    # SAVE: TRAINING SET JSON
    # ----------------------------------------------------------
    output_name = f"trainingSet_SNR_GE_{min_snr}_PS_duration_LE_{max_ps_duration}_and_{depthMin}_LE_Depth_LE_{depthMax}.json"
    output_path = os.path.join(LOG_DIR, output_name)

    save_json(output_path, training_json)
    print(f"\nğŸ’¾ Training Set JSON saved to:\n   {output_path}")
    print(f"ğŸ“¦ Contains {to_training} training-eligible stations.\n")

    # ----------------------------------------------------------
    # SAVE: POTENTIAL TRAINING SET JSON (B1)
    # ----------------------------------------------------------
    potential_path = os.path.join(
        LOG_DIR,
        f"PotentiallyUsedOnTrainingSet_SNR_GE_{min_snr}_PS_duration_GE_{max_ps_duration}_and_{depthMin}_LE_Depth_LE_{depthMax}.json"
    )
    save_json(potential_path, potential_json)

    print(f"ğŸ’¾ Potential Training Set JSON saved to:\n   {potential_path}")
    print(f"ğŸ“¦ Contains {high_snr_and_high_ps_duration_and_depth_in_bounds} potentially useful stations.\n")

    # ----------------------------------------------------------
    # Pretty print report
    # ----------------------------------------------------------
    def print_report_line(label, value, width=110):
        dots = "." * max(1, width - len(label))
        print(f"{label} {dots} {value:>6}")

    print("\nğŸ“Š *** SIGNAL CLASSIFICATION REPORT ***")

    # B1
    label1 = f"âš  POTENCIALY USED : SNR â‰¥ {min_snr} & PS_duration_time > {max_ps_duration} sec & {depthMin}â‰¤Depthâ‰¤{depthMax}"
    print_report_line(label1, high_snr_and_high_ps_duration_and_depth_in_bounds)

    # B2
    label2 = f"âš  NOT USED SET : SNR < {min_snr}"
    print_report_line(label2, low_snr)

    # B3
    label3 = f"âš  NOT USED SET : SNR â‰¥ {min_snr} & clean_event_duration < {min_clean_event_duration} sec"
    print_report_line(label3, high_snr_but_low_clean_event)

    # B4
    label4 = f"âš  NOT USED SET : SNR â‰¥ {min_snr} & (Depth < {depthMin} km or Depth > {depthMax} km)"
    print_report_line(label4, depth_out_of_range)

    print("-" * 110)

    # TRAINING
    label5 = (
        f"âœ” TRAINING SET : SNR â‰¥ {min_snr} & PS_duration_time â‰¤ {max_ps_duration} sec "
        f"& clean_event_duration â‰¥ {min_clean_event_duration} sec "
        f"& {depthMin} km â‰¤ Depth â‰¤ {depthMax} km"
    )
    print_report_line(label5, to_training)

def find_stations_for_ps_fixed(
        min_snr: float,
        max_ps_duration: float,
        min_event_duration: float,
        depth_min: float,
        depth_max: float):
    """
    Creates a PS-FIXED JSON structure by scanning boundaries.json
    and keeping ONLY the stations that satisfy ALL criteria:

    â€¢ minimum_station_snr >= min_snr
    â€¢ peak_segment_duration_time <= max_ps_duration
    â€¢ clean_event_duration_time >= min_event_duration
    â€¢ depth_min <= Depth_km <= depth_max  (Depth from info.json)

    Output:
        Logs/PSfixed_SNR_GE_<min_snr>_PS_LE_<max_ps_duration>_
             CE_GE_<min_event_duration>_DEPTH_<depth_min>_<depth_max>.json
    """

    import os
    from collections import OrderedDict
    from main import LOG_DIR, BASE_DIR

    boundaries_path = os.path.join(LOG_DIR, "boundaries.json")

    if not os.path.exists(boundaries_path):
        print(f"âŒ File not found: {boundaries_path}")
        return

    db = load_json(boundaries_path)
    psfixed_json = {}

    print("\nğŸ” Running Find PS Fixed...")

    # --- Traverse year â†’ event â†’ station ---
    for year, events in db.items():

        if not isinstance(events, dict):
            continue
        if year == "total_nof_stations":
            continue

        for event_name, stations in events.items():

            # --------- Load depth from info.json ---------
            depth_km = None
            info_path = os.path.join(BASE_DIR, str(year), event_name, "info.json")

            if os.path.exists(info_path):
                try:
                    info = load_json(info_path)
                    depth_km = (
                        info.get("depth_km")
                        or info.get("Depth_km")
                        or info.get("depth-km")
                        or info.get("Depth-km")
                    )
                    if depth_km is not None:
                        depth_km = float(depth_km)
                except:
                    depth_km = None

            # If no depth â†’ skip event
            if depth_km is None:
                continue

            # Depth filter
            if not (depth_min <= depth_km <= depth_max):
                continue

            # Now check stations
            for station_name, channels in stations.items():

                if not isinstance(channels, dict):
                    continue

                # ---- SNR check ----
                station_snr = channels.get("minimum_station_snr")
                if station_snr is None:
                    continue
                station_snr = float(station_snr)

                if station_snr < min_snr:
                    continue

                # ---- Z-channel checks ----
                station_is_valid = False

                for ch_name, ch_info in channels.items():

                    if not isinstance(ch_info, dict):
                        continue
                    if not ch_name.endswith("Z"):
                        continue

                    # peak segmentation duration
                    ps = ch_info.get("peak_segment_duration_time")
                    if ps is None:
                        continue
                    try:
                        ps = float(ps)
                    except:
                        continue
                    if ps > max_ps_duration:
                        continue

                    # clean event duration
                    ce = ch_info.get("clean_event_duration_time")
                    if ce is None:
                        continue
                    try:
                        ce = float(ce)
                    except:
                        continue
                    if ce < min_event_duration:
                        continue

                    # If we reach here, channel is valid
                    station_is_valid = True
                    break

                # Save station subtree if valid
                if station_is_valid:
                    year_dict = psfixed_json.setdefault(year, {})
                    event_dict = year_dict.setdefault(event_name, {})
                    event_dict[station_name] = channels

    # ------------------------------------------------------------------
    # COUNT total number of Z-channels (AFTER filtering)
    # ------------------------------------------------------------------
    total_nof_stations = 0
    for year, events in psfixed_json.items():
        if year == "total_nof_stations":
            continue
        for event_name, stations in events.items():
            for station_name, channels in stations.items():
                for ch_name, ch_info in channels.items():
                    if isinstance(ch_info, dict) and ch_name.endswith("Z"):
                        total_nof_stations += 1

    # ----------------------------------------------------------
    # REORDER JSON SO total_nof_stations APPEARS FIRST
    # ----------------------------------------------------------
    ordered_output = OrderedDict()
    ordered_output["total_nof_stations"] = total_nof_stations

    for key, val in psfixed_json.items():
        if key != "total_nof_stations":
            ordered_output[key] = val

    # ----------------------------------------------------------
    # SAVE RESULT JSON
    # ----------------------------------------------------------
    output_name = (
        f"StationsForPsFixed_{min_snr}_{max_ps_duration}_"
        f"({min_event_duration}_({depth_min}-{depth_max}).json"
    )
    output_path = os.path.join(LOG_DIR, output_name)

    save_json(output_path, ordered_output)

    print(f"\nğŸ’¾ PS-FIXED JSON saved to:")
    print(f"   {output_path}")

    # ----------------------------------------------------------
    # Count total stations
    # ----------------------------------------------------------
    total_stations = sum(
        len(stations) for (year, events) in psfixed_json.items()
        if isinstance(events, dict)
        for (event_name, stations) in events.items()
    )

    print(f"ğŸ“¦ Total stations included: {total_stations}")
    print(f"ğŸ§ Total Z-channels included: {total_nof_stations}\n")

    return ordered_output

def find_stations_for_ps_variants_and_clean_events(
        min_snr: float,
        max_ps_duration: float,
        depth_min: float,
        depth_max: float):
    """
    Creates a PS-VARIANT JSON structure by scanning boundaries.json
    and keeping ONLY the stations that satisfy ALL criteria:

    â€¢ minimum_station_snr >= min_snr
    â€¢ peak_segment_duration_time <= max_ps_duration
    â€¢ depth_min <= Depth_km <= depth_max  (Depth from info.json)
    â€¢ At least one Z-channel satisfies the above

    Output:
        Logs/PSvariant_SNR_GE_<min_snr>_PS_LE_<max_ps_duration>_
             DEPTH_<depth_min>_<depth_max>.json
    """

    import os
    from collections import OrderedDict
    from main import LOG_DIR, BASE_DIR

    boundaries_path = os.path.join(LOG_DIR, "boundaries.json")

    if not os.path.exists(boundaries_path):
        print(f"âŒ File not found: {boundaries_path}")
        return

    db = load_json(boundaries_path)
    psvariant_json = {}

    print("\nğŸ” Running Find PS Variant...")

    # ------------------------------------------------------------
    # Traverse year â†’ event â†’ station
    # ------------------------------------------------------------
    for year, events in db.items():

        if not isinstance(events, dict):
            continue
        if year == "total_nof_stations":
            continue

        for event_name, stations in events.items():

            # --------- Load depth from info.json ---------
            depth_km = None
            info_path = os.path.join(BASE_DIR, str(year), event_name, "info.json")

            if os.path.exists(info_path):
                try:
                    info = load_json(info_path)
                    depth_km = (
                        info.get("depth_km")
                        or info.get("Depth_km")
                        or info.get("depth-km")
                        or info.get("Depth-km")
                    )
                    if depth_km is not None:
                        depth_km = float(depth_km)
                except:
                    depth_km = None

            if depth_km is None:
                continue

            # depth filtering
            if not (depth_min <= depth_km <= depth_max):
                continue

            # ------------------------------------------------------------
            # Now evaluate each station
            # ------------------------------------------------------------
            for station_name, channels in stations.items():

                if not isinstance(channels, dict):
                    continue

                # SNR check
                station_snr = channels.get("minimum_station_snr")
                if station_snr is None:
                    continue
                station_snr = float(station_snr)

                if station_snr < min_snr:
                    continue

                # Z-channel validation
                station_is_valid = False

                for ch_name, ch_info in channels.items():

                    if not isinstance(ch_info, dict):
                        continue
                    if not ch_name.endswith("Z"):
                        continue

                    # peak segmentation duration
                    ps = ch_info.get("peak_segment_duration_time")
                    if ps is None:
                        continue
                    try:
                        ps = float(ps)
                    except:
                        continue

                    if ps > max_ps_duration:
                        continue

                    # If reached here, channel is valid
                    station_is_valid = True
                    break

                if station_is_valid:
                    year_dict = psvariant_json.setdefault(year, {})
                    event_dict = year_dict.setdefault(event_name, {})
                    event_dict[station_name] = channels

    # ------------------------------------------------------------
    # Count total Z-channels in output dataset
    # ------------------------------------------------------------
    total_nof_stations = 0
    for year, events in psvariant_json.items():
        for event_name, stations in events.items():
            for station_name, channels in stations.items():
                for ch_name, ch_info in channels.items():
                    if isinstance(ch_info, dict) and ch_name.endswith("Z"):
                        total_nof_stations += 1

    # ------------------------------------------------------------
    # REORDER JSON so that total_nof_stations is FIRST
    # ------------------------------------------------------------
    ordered_output = OrderedDict()
    ordered_output["total_nof_stations"] = total_nof_stations

    for key, val in psvariant_json.items():
        ordered_output[key] = val

    # ------------------------------------------------------------
    # SAVE RESULT FILE
    # ------------------------------------------------------------
    output_name = (
        f"StationsForPsVariantsAndCleanEvents_{min_snr}_{max_ps_duration}_"
        f"({depth_min}-{depth_max}).json"
    )

    output_path = os.path.join(LOG_DIR, output_name)
    save_json(output_path, ordered_output)

    print(f"\nğŸ’¾ PS-VARIANT JSON saved to:")
    print(f"   {output_path}")
    print(f"ğŸ“¦ Total Z-channels included: {total_nof_stations}\n")

    return ordered_output


# ==========================================================
if __name__ == "__main__":
    #find_boundaries()

    find_stations_for_ps_fixed(5, 30, 30, 1,24)
    find_stations_for_ps_variants_and_clean_events(5, 30, 1, 24)
    #create_cutted_signal_files(5, 30)
